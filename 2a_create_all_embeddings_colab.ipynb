{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2a (Unified): Create All Sentence Embeddings\n",
    "\n",
    "This notebook loads the preprocessed data from Google Drive and converts **both original and injected sentences** to embeddings.\n",
    "\n",
    "**Advantages:**\n",
    "- Generate both original and injected embeddings in one run\n",
    "- Guaranteed same data split for both\n",
    "- Save separately for flexible use later\n",
    "- More efficient than running two separate notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"✓ Google Drive mounted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== IMPORTANT: SET THIS PATH ====================\n",
    "# Path to your preprocessed data directory\n",
    "# Example: \"/content/drive/MyDrive/RNN_Preprocessed_Data/20231203_143022\"\n",
    "PREPROCESSED_DATA_DIR = \"/content/drive/MyDrive/RNN_Preprocessed_Data/20251203_133243\"\n",
    "\n",
    "# ==================================================================\n",
    "\n",
    "# Output directory for embeddings\n",
    "EMBEDDINGS_OUTPUT_DIR = os.path.join(\n",
    "    \"/content/drive/MyDrive/RNN_All_Embeddings\",\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    ")\n",
    "os.makedirs(EMBEDDINGS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Preprocessed data directory: {PREPROCESSED_DATA_DIR}\")\n",
    "print(f\"Embeddings will be saved to: {EMBEDDINGS_OUTPUT_DIR}\")\n",
    "\n",
    "# Batch Size for embedding extraction\n",
    "EMBEDDING_BATCH_SIZE = 32\n",
    "\n",
    "# HuggingFace Token\n",
    "hf_token = \"\"  # Add your token here if needed\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Embedding Batch Size: {EMBEDDING_BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "preprocessed_file = os.path.join(PREPROCESSED_DATA_DIR, \"preprocessed_data.pkl\")\n",
    "metadata_file = os.path.join(PREPROCESSED_DATA_DIR, \"metadata.json\")\n",
    "\n",
    "print(f\"Loading preprocessed data from: {preprocessed_file}\")\n",
    "\n",
    "if not os.path.exists(preprocessed_file):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Preprocessed data not found at: {preprocessed_file}\\n\"\n",
    "        f\"Please run '1_preprocess_dataset_colab.ipynb' first and update PREPROCESSED_DATA_DIR.\"\n",
    "    )\n",
    "\n",
    "with open(preprocessed_file, 'rb') as f:\n",
    "    preprocessed_data = pickle.load(f)\n",
    "\n",
    "print(f\"✓ Loaded {len(preprocessed_data)} preprocessed documents\")\n",
    "\n",
    "# Load metadata\n",
    "if os.path.exists(metadata_file):\n",
    "    with open(metadata_file, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    print(f\"✓ Metadata loaded\")\n",
    "    print(f\"  Preprocessing date: {metadata['preprocessing_timestamp']}\")\n",
    "    print(f\"  Total documents: {metadata['total_documents']}\")\n",
    "    print(f\"  Total sentences: {metadata['total_sentences']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Device and Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Login\n",
    "if hf_token:\n",
    "    try:\n",
    "        login(token=hf_token)\n",
    "        print(\"✓ Logged in to HuggingFace\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Login failed. {e}\")\n",
    "\n",
    "# Device Configuration\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\n✓ Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLM Model (for embeddings)\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "print(f\"\\nLoading {model_id} for embeddings...\")\n",
    "\n",
    "try:\n",
    "    tokenizer_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    model_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        output_hidden_states=True,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    print(f\"✓ Model loaded successfully\")\n",
    "    print(f\"  Hidden size: {llm_model.config.hidden_size}\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR loading model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(texts):\n",
    "    \"\"\"\n",
    "    Get embeddings for a batch of sentences using mean pooling.\n",
    "\n",
    "    Args:\n",
    "        texts: List of strings\n",
    "\n",
    "    Returns:\n",
    "        embeddings: (batch_size, hidden_size)\n",
    "    \"\"\"\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "\n",
    "    if not texts or all(not t for t in texts):\n",
    "        return torch.zeros((len(texts) if texts else 1, llm_model.config.hidden_size), device=device)\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model(**inputs)\n",
    "\n",
    "    last_hidden = outputs.hidden_states[-1]\n",
    "    mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "\n",
    "    sum_emb = torch.sum(last_hidden * mask, dim=1)\n",
    "    counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "    mean_emb = sum_emb / counts\n",
    "\n",
    "    return mean_emb\n",
    "\n",
    "print(\"✓ Embedding extraction function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Split Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test (60/20/20)\n",
    "train_data, temp_data = train_test_split(preprocessed_data, test_size=0.4, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"  Train: {len(train_data)} documents\")\n",
    "print(f\"  Val:   {len(val_data)} documents\")\n",
    "print(f\"  Test:  {len(test_data)} documents\")\n",
    "\n",
    "# Save split info with doc_ids for reproducibility\n",
    "split_info = {\n",
    "    \"total_docs\": len(preprocessed_data),\n",
    "    \"train_docs\": len(train_data),\n",
    "    \"val_docs\": len(val_data),\n",
    "    \"test_docs\": len(test_data),\n",
    "    \"split_ratios\": \"60/20/20\",\n",
    "    \"random_seed\": 42,\n",
    "    \"train_doc_ids\": [doc['doc_id'] for doc in train_data],\n",
    "    \"val_doc_ids\": [doc['doc_id'] for doc in val_data],\n",
    "    \"test_doc_ids\": [doc['doc_id'] for doc in test_data],\n",
    "    \"timestamp\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(os.path.join(EMBEDDINGS_OUTPUT_DIR, \"data_split_info.json\"), \"w\") as f:\n",
    "    json.dump(split_info, f, indent=2)\n",
    "\n",
    "print(f\"✓ Split info saved to Google Drive (with doc_ids for reproducibility)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Convert Sentences to Embeddings (Original + Injected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_embeddings(data_split, split_name):\n",
    "    \"\"\"\n",
    "    Convert both original and injected sentences to embedding sequences.\n",
    "    \n",
    "    This function generates TWO separate embedding sequences per document:\n",
    "    1. Original sentence embeddings\n",
    "    2. Injected sentence embeddings\n",
    "\n",
    "    Args:\n",
    "        data_split: List of preprocessed documents\n",
    "        split_name: Name of split (for progress bar)\n",
    "\n",
    "    Returns:\n",
    "        original_sequences: List of (seq_len, embedding_dim) tensors\n",
    "        injected_sequences: List of (seq_len, embedding_dim) tensors\n",
    "        labels: List of labels\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Converting {split_name} documents to embeddings\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    all_original_sequences = []\n",
    "    all_injected_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    for doc in tqdm(data_split, desc=f\"[{split_name}] Embedding\"):\n",
    "        original_sentences = doc['original_sentences']\n",
    "        injected_sentences = doc['injected_sentences']\n",
    "        label = doc['label']\n",
    "\n",
    "        # Extract embeddings in batches\n",
    "        orig_embeddings_list = []\n",
    "        inj_embeddings_list = []\n",
    "\n",
    "        for i in range(0, len(original_sentences), EMBEDDING_BATCH_SIZE):\n",
    "            batch_orig = original_sentences[i:i+EMBEDDING_BATCH_SIZE]\n",
    "            batch_inj = injected_sentences[i:i+EMBEDDING_BATCH_SIZE]\n",
    "\n",
    "            emb_orig = get_sentence_embedding(batch_orig).cpu()\n",
    "            emb_inj = get_sentence_embedding(batch_inj).cpu()\n",
    "\n",
    "            orig_embeddings_list.append(emb_orig)\n",
    "            inj_embeddings_list.append(emb_inj)\n",
    "\n",
    "        # Concatenate all batches\n",
    "        orig_embeddings = torch.cat(orig_embeddings_list, dim=0)\n",
    "        inj_embeddings = torch.cat(inj_embeddings_list, dim=0)\n",
    "\n",
    "        # Store separately (NOT concatenated)\n",
    "        all_original_sequences.append(orig_embeddings)\n",
    "        all_injected_sequences.append(inj_embeddings)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    print(f\"[{split_name}] Created {len(all_original_sequences)} original sequences\")\n",
    "    print(f\"[{split_name}] Created {len(all_injected_sequences)} injected sequences\")\n",
    "    return all_original_sequences, all_injected_sequences, all_labels\n",
    "\n",
    "print(\"✓ Conversion function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Embeddings for All Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all splits to embeddings\n",
    "train_orig, train_inj, train_labels = convert_to_embeddings(train_data, \"Train\")\n",
    "val_orig, val_inj, val_labels = convert_to_embeddings(val_data, \"Val\")\n",
    "test_orig, test_inj, test_labels = convert_to_embeddings(test_data, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Embeddings to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Saving all embeddings to Google Drive\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Save embeddings with BOTH original and injected sequences\n",
    "embeddings_data = {\n",
    "    'train': {\n",
    "        'original_sequences': train_orig,\n",
    "        'injected_sequences': train_inj,\n",
    "        'labels': train_labels\n",
    "    },\n",
    "    'val': {\n",
    "        'original_sequences': val_orig,\n",
    "        'injected_sequences': val_inj,\n",
    "        'labels': val_labels\n",
    "    },\n",
    "    'test': {\n",
    "        'original_sequences': test_orig,\n",
    "        'injected_sequences': test_inj,\n",
    "        'labels': test_labels\n",
    "    }\n",
    "}\n",
    "\n",
    "embeddings_file = os.path.join(EMBEDDINGS_OUTPUT_DIR, \"embeddings.pkl\")\n",
    "with open(embeddings_file, 'wb') as f:\n",
    "    pickle.dump(embeddings_data, f)\n",
    "\n",
    "print(f\"✓ All embeddings saved to: {embeddings_file}\")\n",
    "\n",
    "# Calculate file size\n",
    "file_size_mb = os.path.getsize(embeddings_file) / (1024 * 1024)\n",
    "print(f\"  File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata\n",
    "embeddings_metadata = {\n",
    "    \"creation_timestamp\": datetime.now().isoformat(),\n",
    "    \"preprocessed_data_dir\": PREPROCESSED_DATA_DIR,\n",
    "    \"model_id\": model_id,\n",
    "    \"embedding_dim\": llm_model.config.hidden_size,\n",
    "    \"embedding_batch_size\": EMBEDDING_BATCH_SIZE,\n",
    "    \"train_samples\": len(train_orig),\n",
    "    \"val_samples\": len(val_orig),\n",
    "    \"test_samples\": len(test_orig),\n",
    "    \"original_embedding_dim\": train_orig[0].shape[1] if train_orig else 0,\n",
    "    \"injected_embedding_dim\": train_inj[0].shape[1] if train_inj else 0,\n",
    "    \"embedding_types\": {\n",
    "        \"original_sequences\": \"Original sentence embeddings (seq_len, embedding_dim)\",\n",
    "        \"injected_sequences\": \"Injected sentence embeddings (seq_len, embedding_dim)\"\n",
    "    },\n",
    "    \"device\": device,\n",
    "    \"note\": \"Contains BOTH original and injected embeddings separately. Use either independently or combine as needed.\"\n",
    "}\n",
    "\n",
    "metadata_file = os.path.join(EMBEDDINGS_OUTPUT_DIR, \"embeddings_metadata.json\")\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(embeddings_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Metadata saved to: {metadata_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Verification and Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify shapes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"VERIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nTrain set first document:\")\n",
    "print(f\"  Original embedding shape: {train_orig[0].shape}\")\n",
    "print(f\"  Injected embedding shape: {train_inj[0].shape}\")\n",
    "print(f\"  Label: {train_labels[0]}\")\n",
    "\n",
    "print(f\"\\nUsage Examples:\")\n",
    "print(f\"  1. Use original only: data['train']['original_sequences']\")\n",
    "print(f\"  2. Use injected only: data['train']['injected_sequences']\")\n",
    "print(f\"  3. Concatenate both: torch.cat([orig, inj], dim=1) -> (seq_len, {train_orig[0].shape[1]*2})\")\n",
    "print(f\"  4. Use as separate inputs in multi-input model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EMBEDDING GENERATION COMPLETE - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\n✓ All embeddings saved to Google Drive:\")\n",
    "print(f\"  {EMBEDDINGS_OUTPUT_DIR}\\n\")\n",
    "print(f\"Saved files:\")\n",
    "print(f\"  1. embeddings.pkl - All embedding tensors\")\n",
    "print(f\"  2. embeddings_metadata.json - Embedding metadata\")\n",
    "print(f\"  3. data_split_info.json - Split information with doc_ids\\n\")\n",
    "print(f\"Embedding Statistics:\")\n",
    "print(f\"  Train samples: {len(train_orig)}\")\n",
    "print(f\"  Val samples:   {len(val_orig)}\")\n",
    "print(f\"  Test samples:  {len(test_orig)}\")\n",
    "print(f\"  Original embedding dim: {train_orig[0].shape[1] if train_orig else 0}\")\n",
    "print(f\"  Injected embedding dim: {train_inj[0].shape[1] if train_inj else 0}\\n\")\n",
    "print(f\"Data Structure:\")\n",
    "print(f\"  embeddings_data['train']['original_sequences'] - List of original embeddings\")\n",
    "print(f\"  embeddings_data['train']['injected_sequences'] - List of injected embeddings\")\n",
    "print(f\"  embeddings_data['train']['labels'] - List of labels\\n\")\n",
    "print(f\"Next step:\")\n",
    "print(f\"  Use this data for training models\")\n",
    "print(f\"  Can use original, injected, or both depending on your model architecture\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
