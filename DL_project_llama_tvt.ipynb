{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"19a1fd7052ce4efea740672cac4c2644":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_685a68dbdc2a4c75a0559693806b475e","IPY_MODEL_5ff96ebe8090410a9545cd629ea3f4a3","IPY_MODEL_1650b18286a24aa28052849d3c16af6f"],"layout":"IPY_MODEL_644b898ca1a041fc82ee08e088ca49ab"}},"685a68dbdc2a4c75a0559693806b475e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8bd61ab0bf564411be7bca2c5c09c39d","placeholder":"​","style":"IPY_MODEL_e020b50bfd07491dbbbac048aadb3b48","value":"tokenizer_config.json: 100%"}},"5ff96ebe8090410a9545cd629ea3f4a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b8b08a567113461493844468ee32f30b","max":54528,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9bb964b0c53045678039cb9e194b7c4a","value":54528}},"1650b18286a24aa28052849d3c16af6f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04e6e9919f414323a4997e9c73ef5c23","placeholder":"​","style":"IPY_MODEL_ede25cd8025e476986156efee6679602","value":" 54.5k/54.5k [00:00&lt;00:00, 4.58MB/s]"}},"644b898ca1a041fc82ee08e088ca49ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bd61ab0bf564411be7bca2c5c09c39d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e020b50bfd07491dbbbac048aadb3b48":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8b08a567113461493844468ee32f30b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bb964b0c53045678039cb9e194b7c4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04e6e9919f414323a4997e9c73ef5c23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ede25cd8025e476986156efee6679602":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"af11dad551c04cbd9910030671f3c247":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_29c76329247b4d2fa1bae86acc83275d","IPY_MODEL_a79aed3dfa3e4988a67e0ff8a1a759de","IPY_MODEL_5bed9ef2855d401d95e06b297d7f7aba"],"layout":"IPY_MODEL_1ddf94f0488f42ed98eacba8b6e394aa"}},"29c76329247b4d2fa1bae86acc83275d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a00e3e37310a409291924f88fba125cf","placeholder":"​","style":"IPY_MODEL_3c75b7c0e6a0429c8213d091b8e592a9","value":"tokenizer.json: 100%"}},"a79aed3dfa3e4988a67e0ff8a1a759de":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9ee7b294a014b2992af06bb3d7f72ec","max":9085657,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bdbfb5bf6dd04ebd8772fd0cfec233e3","value":9085657}},"5bed9ef2855d401d95e06b297d7f7aba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55a579e25f4e455bb3b192e1b995663b","placeholder":"​","style":"IPY_MODEL_55a806ff337b4cc6a28ac440a9d0d99e","value":" 9.09M/9.09M [00:01&lt;00:00, 5.97MB/s]"}},"1ddf94f0488f42ed98eacba8b6e394aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a00e3e37310a409291924f88fba125cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c75b7c0e6a0429c8213d091b8e592a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a9ee7b294a014b2992af06bb3d7f72ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bdbfb5bf6dd04ebd8772fd0cfec233e3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55a579e25f4e455bb3b192e1b995663b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55a806ff337b4cc6a28ac440a9d0d99e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"812d01ce56094378b7b3cceabab0628b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b0729cf5f60046028782ef98da45082e","IPY_MODEL_b48bc08f76534476bb95e337c4c53cb3","IPY_MODEL_a02fd824865b4ad28ff1664b6d560282"],"layout":"IPY_MODEL_44d7cf84773b4e379447211a19c9a5f7"}},"b0729cf5f60046028782ef98da45082e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d603aab863d545c3998785fcc0607b20","placeholder":"​","style":"IPY_MODEL_568ce285bf854fc8a25c8ded007e4062","value":"special_tokens_map.json: 100%"}},"b48bc08f76534476bb95e337c4c53cb3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e776a81d48cb422cbaf2391a2d17b0f9","max":296,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58b29e8acb884a948e9c2b799268fee8","value":296}},"a02fd824865b4ad28ff1664b6d560282":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1fd9eec86e046d2ac037b663cba0020","placeholder":"​","style":"IPY_MODEL_97fb424638c94dc88c67527f78b61154","value":" 296/296 [00:00&lt;00:00, 33.3kB/s]"}},"44d7cf84773b4e379447211a19c9a5f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d603aab863d545c3998785fcc0607b20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"568ce285bf854fc8a25c8ded007e4062":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e776a81d48cb422cbaf2391a2d17b0f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58b29e8acb884a948e9c2b799268fee8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e1fd9eec86e046d2ac037b663cba0020":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97fb424638c94dc88c67527f78b61154":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b535ca9366f8424aad3930114fba9800":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_886cd1647bd94b6f9249b3003be64228","IPY_MODEL_06784eb3612544b9bffefce52fb2e733","IPY_MODEL_935976cee55a427a952c6488952570a9"],"layout":"IPY_MODEL_3eb8171bbd3b418e971e66a3685f058e"}},"886cd1647bd94b6f9249b3003be64228":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_25a06268937444018386741479f49fa6","placeholder":"​","style":"IPY_MODEL_420b12a1172b4aa6844008f10c6b409d","value":"config.json: 100%"}},"06784eb3612544b9bffefce52fb2e733":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1eda40b2c934f288ae930556b9c8de7","max":877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_046be9070dd74c3096d940e71f942fc3","value":877}},"935976cee55a427a952c6488952570a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b004b255ab954eea83f565f6a42d4cbc","placeholder":"​","style":"IPY_MODEL_1db0c963cd374ac9bd37818112fa104e","value":" 877/877 [00:00&lt;00:00, 98.7kB/s]"}},"3eb8171bbd3b418e971e66a3685f058e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a06268937444018386741479f49fa6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"420b12a1172b4aa6844008f10c6b409d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1eda40b2c934f288ae930556b9c8de7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"046be9070dd74c3096d940e71f942fc3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b004b255ab954eea83f565f6a42d4cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1db0c963cd374ac9bd37818112fa104e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbb8ba9f0bf34e688311340e18d4ee84":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_35b32266fa22441596870177f53fa45e","IPY_MODEL_d0a32c4714414aa88cae5cb3d2f49432","IPY_MODEL_ccba3e39564c40d4aeea4420aaf71ced"],"layout":"IPY_MODEL_00ae86b0da12432b94a6301bc33fd4d8"}},"35b32266fa22441596870177f53fa45e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d6bf50a4ddfe4730a2df289af70f8fc9","placeholder":"​","style":"IPY_MODEL_8639a51780664ca28620f8afe7ea1daa","value":"model.safetensors: 100%"}},"d0a32c4714414aa88cae5cb3d2f49432":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b30e8370b9514c2db70e0ab4127b1fe6","max":2471645608,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26bbffe8d6564d20a0c8faf7608d9c77","value":2471645608}},"ccba3e39564c40d4aeea4420aaf71ced":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0691e9b42f2c4ee3afedee874402c92d","placeholder":"​","style":"IPY_MODEL_28ed1eedf8d440ef9acafeeb1bebc5e0","value":" 2.47G/2.47G [00:19&lt;00:00, 241MB/s]"}},"00ae86b0da12432b94a6301bc33fd4d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6bf50a4ddfe4730a2df289af70f8fc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8639a51780664ca28620f8afe7ea1daa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b30e8370b9514c2db70e0ab4127b1fe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26bbffe8d6564d20a0c8faf7608d9c77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0691e9b42f2c4ee3afedee874402c92d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28ed1eedf8d440ef9acafeeb1bebc5e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5e594be38e64d52b7ffa3e1197b5116":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_231bab54425e418a85d865fcae089c37","IPY_MODEL_c9646c38fab04cfa8386fdb5deefd31e","IPY_MODEL_80ab1338132845d0adff998055f0f671"],"layout":"IPY_MODEL_c6e70c4149a14ea58a47fe1683595ff9"}},"231bab54425e418a85d865fcae089c37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20eeea2c14b54616924c4d98c7a13042","placeholder":"​","style":"IPY_MODEL_a4672007ce9641d4868679bda9147214","value":"generation_config.json: 100%"}},"c9646c38fab04cfa8386fdb5deefd31e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_85c8fee0cd79450a975c08076292adf1","max":189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4620ed06be64f0e8d1e086533e741ee","value":189}},"80ab1338132845d0adff998055f0f671":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_767b67244bc54c0ea7a4d2c6ffe28460","placeholder":"​","style":"IPY_MODEL_8457777ac4584ff391aa8c8b3939b75a","value":" 189/189 [00:00&lt;00:00, 23.2kB/s]"}},"c6e70c4149a14ea58a47fe1683595ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20eeea2c14b54616924c4d98c7a13042":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4672007ce9641d4868679bda9147214":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"85c8fee0cd79450a975c08076292adf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c4620ed06be64f0e8d1e086533e741ee":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"767b67244bc54c0ea7a4d2c6ffe28460":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8457777ac4584ff391aa8c8b3939b75a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f03b61f321e46e492bca6a98173edc2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4135743d136440a78e8dc84173c28a3c","IPY_MODEL_f988542e2a7843c794efb833c4c4a3b5","IPY_MODEL_80609f84948f43719b926482de40237f"],"layout":"IPY_MODEL_c43e11e041a44c87be5276e44921f69f"}},"4135743d136440a78e8dc84173c28a3c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b56b3b1042646289c151c128349ddf1","placeholder":"​","style":"IPY_MODEL_88e1dd2ac13e4a7d89c53de88b013126","value":"README.md: "}},"f988542e2a7843c794efb833c4c4a3b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_665edd3c61634e03adf4923eba813d5c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13eb545b08a046fbbd633eaaca02f40b","value":1}},"80609f84948f43719b926482de40237f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b0f8eb959614d959a9b6c1757312dd5","placeholder":"​","style":"IPY_MODEL_b8c5993f4f844acbbca2a60bb1b68d9f","value":" 2.50k/? [00:00&lt;00:00, 219kB/s]"}},"c43e11e041a44c87be5276e44921f69f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b56b3b1042646289c151c128349ddf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88e1dd2ac13e4a7d89c53de88b013126":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"665edd3c61634e03adf4923eba813d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"13eb545b08a046fbbd633eaaca02f40b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4b0f8eb959614d959a9b6c1757312dd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8c5993f4f844acbbca2a60bb1b68d9f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a8a0fe16800d41b591193d68c7e0ee5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3943a7d8cd344dc0b4ba63bff74607db","IPY_MODEL_5e111d423f1f423992972fae0eb635fb","IPY_MODEL_8f155f7b4a444022b430e1831e4e78cb"],"layout":"IPY_MODEL_7f68e82e48914682a5f4d197ef6f461d"}},"3943a7d8cd344dc0b4ba63bff74607db":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8773393456b47958ece9ef93dff8903","placeholder":"​","style":"IPY_MODEL_1ee7cb3d5ba846c3b0602f1e56d84568","value":"data/train-00000-of-00001.parquet: 100%"}},"5e111d423f1f423992972fae0eb635fb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_77d6068c39ce418aabd7fb96eb356d72","max":41730819,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff8602ddf5844838a29709a599102203","value":41730819}},"8f155f7b4a444022b430e1831e4e78cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93fda5810db14697a32d500a60616e24","placeholder":"​","style":"IPY_MODEL_f8551caaf36847ffbd095e4761efcbee","value":" 41.7M/41.7M [00:00&lt;00:00, 51.1MB/s]"}},"7f68e82e48914682a5f4d197ef6f461d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8773393456b47958ece9ef93dff8903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ee7cb3d5ba846c3b0602f1e56d84568":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"77d6068c39ce418aabd7fb96eb356d72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff8602ddf5844838a29709a599102203":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93fda5810db14697a32d500a60616e24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8551caaf36847ffbd095e4761efcbee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"565cfb8124c441eeaec1c0eb9b7e059a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd273ebc4dde48878ad05e17d3a518bb","IPY_MODEL_20b7abd9a8914800b4dbb1a12d4c70dc","IPY_MODEL_239195f73b1e456585e9c63c6e8a291c"],"layout":"IPY_MODEL_43fd9741a2fa4186845503be9d28d3c5"}},"cd273ebc4dde48878ad05e17d3a518bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89e186b2795c41328d0c7cdd575f2636","placeholder":"​","style":"IPY_MODEL_4a0d985689e74cefbcaa6451d07a2a8e","value":"Generating train split: 100%"}},"20b7abd9a8914800b4dbb1a12d4c70dc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c14dc79636ef482381c92b630017630e","max":3000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae3c02164fc948acbce50bfe9b414565","value":3000}},"239195f73b1e456585e9c63c6e8a291c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_932c8d19d7214950b1a93c8747c592a9","placeholder":"​","style":"IPY_MODEL_485ff042e4384049aff0057b3930a609","value":" 3000/3000 [00:00&lt;00:00, 6024.31 examples/s]"}},"43fd9741a2fa4186845503be9d28d3c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89e186b2795c41328d0c7cdd575f2636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a0d985689e74cefbcaa6451d07a2a8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c14dc79636ef482381c92b630017630e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae3c02164fc948acbce50bfe9b414565":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"932c8d19d7214950b1a93c8747c592a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"485ff042e4384049aff0057b3930a609":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import nltk\n","import logging\n","import sys\n","import os\n","from tqdm import tqdm\n","import random\n","\n","# --- Setup & Configuration ---\n","NUM_ROWS_TO_USE = None  # Strict limit: Process exactly this many rows (e.g., 100 rows -> 200 samples)\n","\n","# 1. Login\n","hf_token = \"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\"\n","\n","if hf_token:\n","    try:\n","        login(token=hf_token)\n","    except Exception as e:\n","        print(f\"Warning: Login failed. {e}\")\n","else:\n","    print(\"No HF token provided. Running anonymously.\")\n","\n","# 2. Model Configuration\n","model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if torch.backends.mps.is_available():\n","    device = \"mps\"\n","\n","print(f\"Loading {model_id} on {device}...\")\n","\n","try:\n","    # Load Tokenizer\n","    tokenizer_kwargs = {\"token\": hf_token} if hf_token else {}\n","    tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"left\"\n","\n","    # Load Model\n","    # Note: Using 'torch_dtype' as it is the correct argument for Hugging Face AutoModel.\n","    model_kwargs = {\"token\": hf_token} if hf_token else {}\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        output_hidden_states=True,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"CRITICAL ERROR loading model: {e}\")\n","    sys.exit(1)\n","\n","# Initialize Generation Pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",\n","    pad_token_id=tokenizer.eos_token_id,\n","    batch_size=32\n",")\n","\n","# NLTK Setup\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","# --- MLP Classifier Definition ---\n","\n","class ArtifactDetectorMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(ArtifactDetectorMLP, self).__init__()\n","        # Input dim will be 2 * Hidden_Size (Original + Injected)\n","        self.layer1 = nn.Linear(input_dim, 512)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","        self.layer2 = nn.Linear(512, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.layer2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# --- Helper Functions ---\n","\n","def get_sentences(text):\n","    \"\"\"Returns ALL sentences from the text (No slicing).\"\"\"\n","    if not text or not isinstance(text, str):\n","        return []\n","    return nltk.tokenize.sent_tokenize(text)\n","\n","def run_transformation_pipeline(texts, mode, description=\"Transforming\"):\n","    \"\"\"\n","    Runs the pipeline using a GENERATOR to maximize GPU throughput.\n","    \"\"\"\n","    tokenizer.padding_side = \"left\"\n","\n","    if mode == \"reduce\":\n","        system_prompt = (\n","            \"Task: Simplify the sentence. Keep the main Subject, Verb, and Object.\\n\"\n","            \"Rule: Do not change the meaning. Do not loop. Remove extra details.\\n\"\n","            \"Input Sentence: \"\n","        )\n","    elif mode == \"inject\":\n","        system_prompt = (\n","            \"Task: Rewrite the sentence to be more descriptive and vivid.\\n\"\n","            \"Rule: Add adjectives and adverbs. Keep the original meaning.\\n\"\n","            \"Input Sentence: \"\n","        )\n","\n","    # 1. Prepare Inputs\n","    chat_inputs = [\n","        [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": system_prompt + t}\n","        ] for t in texts\n","    ]\n","\n","    # 2. Define Generator\n","    def input_generator():\n","        for item in chat_inputs:\n","            yield item\n","\n","    results = []\n","\n","    # 3. Iterate over the pipeline output\n","    pipeline_iterator = pipe(\n","        input_generator(),\n","        batch_size=32,\n","        max_new_tokens=64,\n","        do_sample=False\n","    )\n","\n","    # Use tqdm on the iterator for visual feedback\n","    for out in tqdm(pipeline_iterator, total=len(texts), desc=description):\n","        try:\n","            generated_conv = out[0]['generated_text']\n","            content = generated_conv[-1]['content']\n","\n","            if \"Input Sentence:\" in content:\n","                content = content.split(\"Input Sentence:\")[-1].strip()\n","            results.append(content.strip())\n","        except Exception:\n","            results.append(\"\")\n","\n","    return results\n","\n","def get_embedding_smart_chunking(text, max_tokens=512):\n","    \"\"\"\n","    Calculates document embedding using Sentence-Aware Smart Chunking.\n","    \"\"\"\n","    if not text:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    sentences = get_sentences(text)\n","    if not sentences:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    # 1. Create Chunks\n","    chunks = []\n","    current_chunk = []\n","    current_length = 0\n","\n","    tokenizer.padding_side = \"right\"\n","\n","    for sent in sentences:\n","        token_len = len(tokenizer.encode(sent, add_special_tokens=False))\n","\n","        if current_length + token_len > max_tokens and current_chunk:\n","            chunks.append(\" \".join(current_chunk))\n","            current_chunk = []\n","            current_length = 0\n","\n","        current_chunk.append(sent)\n","        current_length += token_len\n","\n","    if current_chunk:\n","        chunks.append(\" \".join(current_chunk))\n","\n","    # 2. Embed Chunks\n","    chunk_embeddings = []\n","\n","    batch_size = 4\n","    for i in range(0, len(chunks), batch_size):\n","        batch_texts = chunks[i : i + batch_size]\n","\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_tokens\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        last_hidden = outputs.hidden_states[-1]\n","        mask = inputs['attention_mask'].unsqueeze(-1)\n","\n","        sum_emb = torch.sum(last_hidden * mask, dim=1)\n","        counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n","        mean_emb = sum_emb / counts\n","\n","        chunk_embeddings.append(mean_emb)\n","\n","    # 3. Average across chunks\n","    if not chunk_embeddings:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    all_chunks_tensor = torch.cat(chunk_embeddings, dim=0)\n","    doc_embedding = torch.mean(all_chunks_tensor, dim=0, keepdim=True)\n","\n","    return doc_embedding\n","\n","# --- Dataset Processing (Strict & Balanced) ---\n","\n","def process_full_dataset():\n","    print(f\"\\n Loading FULL Dataset...\")\n","    try:\n","        ds = load_dataset(\"artnitolog/llm-generated-texts\", split=\"train\")\n","    except Exception as e:\n","        print(f\"Error loading dataset: {e}\")\n","        return None, None\n","\n","    # 1. Collect Raw Data (Strict 1:1 Logic)\n","    raw_samples = []\n","\n","    # Exclude metadata columns\n","    excluded_cols = {'id', 'prompt', 'dataset_name', 'classes'}\n","\n","    # Counter for Processed Source Rows\n","    rows_processed_count = 0\n","\n","    print(f\"Collecting valid samples (Target: {NUM_ROWS_TO_USE} Rows)...\")\n","\n","    for row in tqdm(ds, desc=\"Scanning Dataset\"):\n","        # Check Stop Condition\n","        if NUM_ROWS_TO_USE is not None and rows_processed_count >= NUM_ROWS_TO_USE:\n","            break\n","\n","        all_cols = row.keys()\n","\n","        # Identify Human Column\n","        human_col = next((c for c in all_cols if 'human' in c.lower()), None)\n","        if not human_col: continue # Skip if no human text found\n","\n","        # Identify AI Candidate Columns\n","        ai_candidates = []\n","        for col in all_cols:\n","            if col in excluded_cols: continue\n","            if col == human_col: continue\n","\n","            # Ensure the column has content\n","            if row[col] and isinstance(row[col], str) and len(row[col]) > 10:\n","                ai_candidates.append(col)\n","\n","        if not ai_candidates: continue # Skip if no AI text found\n","        if not row[human_col]: continue\n","\n","        # --- Balanced Sampling Logic ---\n","\n","        # 1. Add Human Sample (Label 0)\n","        human_text = row[human_col]\n","        raw_samples.append((human_text, 0))\n","\n","        # 2. Add Random AI Sample (Label 1)\n","        selected_ai_col = random.choice(ai_candidates)\n","        ai_text = row[selected_ai_col]\n","        raw_samples.append((ai_text, 1))\n","\n","        # Increment Row Counter (1 Row = 1 Human + 1 AI)\n","        rows_processed_count += 1\n","\n","    print(f\"\\nValidation:\")\n","    print(f\"Target Rows: {NUM_ROWS_TO_USE}\")\n","    print(f\"Processed Rows: {rows_processed_count}\")\n","    print(f\"Total Samples Collected: {len(raw_samples)} (Should be {2 * rows_processed_count})\")\n","    print(f\"Human Samples: {sum(1 for _, l in raw_samples if l == 0)}\")\n","    print(f\"AI Samples: {sum(1 for _, l in raw_samples if l == 1)}\")\n","\n","    # 2. Flatten into Sentences\n","    all_sentences = []\n","    doc_boundaries = []\n","    current_idx = 0\n","    valid_samples = []\n","\n","    print(\"Tokenizing sentences (FULL TEXT)...\")\n","    for text, label in raw_samples:\n","        sents = get_sentences(text) # ALL sentences\n","        if not sents: continue\n","\n","        all_sentences.extend(sents)\n","        doc_boundaries.append((current_idx, current_idx + len(sents)))\n","        valid_samples.append((text, label))\n","        current_idx += len(sents)\n","\n","    # 3. Batch Transformation\n","    reduced_sentences = run_transformation_pipeline(\n","        all_sentences,\n","        \"reduce\",\n","        description=\"Step 1/2: Reducing\"\n","    )\n","\n","    injected_sentences = run_transformation_pipeline(\n","        reduced_sentences,\n","        \"inject\",\n","        description=\"Step 2/2: Injecting\"\n","    )\n","\n","    # 4. Feature Extraction\n","    print(\"\\nCalculating Smart Chunked Embeddings...\")\n","    features_list = []\n","    labels_list = []\n","\n","    for i, (start, end) in enumerate(tqdm(doc_boundaries, desc=\"Embedding Docs\")):\n","        inj_segment = \" \".join(injected_sentences[start:end])\n","        orig_text = valid_samples[i][0]\n","\n","        emb_orig = get_embedding_smart_chunking(orig_text)\n","        emb_inj = get_embedding_smart_chunking(inj_segment)\n","\n","        feat = torch.cat((emb_orig, emb_inj), dim=1)\n","\n","        features_list.append(feat.cpu())\n","        labels_list.append(valid_samples[i][1])\n","\n","    if not features_list:\n","        return None, None\n","\n","    X = torch.cat(features_list, dim=0)\n","    y = torch.tensor(labels_list, dtype=torch.float32).unsqueeze(1)\n","\n","    return X, y\n","\n","# --- Training & Evaluation ---\n","\n","def train_and_evaluate(X, y):\n","    print(\"\\n Splitting Data (60/20/20)...\")\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","    print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n","\n","    X_train, y_train = X_train.to(device), y_train.to(device)\n","    X_val, y_val = X_val.to(device), y_val.to(device)\n","    X_test, y_test = X_test.to(device), y_test.to(device)\n","\n","    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n","\n","    input_dim = X.shape[1]\n","    mlp = ArtifactDetectorMLP(input_dim).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(mlp.parameters(), lr=0.0005)\n","\n","    epochs = 20\n","    best_val_acc = 0.0\n","\n","    print(\"\\n Starting Training...\")\n","\n","    for epoch in range(epochs):\n","        mlp.train()\n","        train_loss = 0\n","\n","        for batch_X, batch_y in train_loader:\n","            optimizer.zero_grad()\n","            outputs = mlp(batch_X)\n","            loss = criterion(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        mlp.eval()\n","        with torch.no_grad():\n","            val_out = mlp(X_val)\n","            val_loss = criterion(val_out, y_val).item()\n","            val_pred = (val_out > 0.5).float()\n","            val_acc = (val_pred == y_val).sum().item() / len(y_val) * 100\n","\n","        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(mlp.state_dict(), \"best_model.pth\")\n","\n","    print(f\"\\n Training Complete. Best Val Acc: {best_val_acc:.2f}%\")\n","\n","    print(\"\\n Evaluating on Test Set...\")\n","    mlp.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n","    mlp.eval()\n","\n","    with torch.no_grad():\n","        test_out = mlp(X_test)\n","        test_pred = (test_out > 0.5).float()\n","        test_acc = (test_pred == y_test).sum().item() / len(y_test) * 100\n","\n","    print(f\"{'='*30}\")\n","    print(f\"FINAL TEST ACCURACY: {test_acc:.2f}%\")\n","    print(f\"{'='*30}\")\n","\n","    return mlp\n","\n","def main():\n","    # 1. Process Data\n","    X, y = process_full_dataset()\n","\n","    if X is None:\n","        print(\"Data preparation failed.\")\n","        return\n","\n","    # 2. Train & Eval\n","    train_and_evaluate(X, y)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["19a1fd7052ce4efea740672cac4c2644","685a68dbdc2a4c75a0559693806b475e","5ff96ebe8090410a9545cd629ea3f4a3","1650b18286a24aa28052849d3c16af6f","644b898ca1a041fc82ee08e088ca49ab","8bd61ab0bf564411be7bca2c5c09c39d","e020b50bfd07491dbbbac048aadb3b48","b8b08a567113461493844468ee32f30b","9bb964b0c53045678039cb9e194b7c4a","04e6e9919f414323a4997e9c73ef5c23","ede25cd8025e476986156efee6679602","af11dad551c04cbd9910030671f3c247","29c76329247b4d2fa1bae86acc83275d","a79aed3dfa3e4988a67e0ff8a1a759de","5bed9ef2855d401d95e06b297d7f7aba","1ddf94f0488f42ed98eacba8b6e394aa","a00e3e37310a409291924f88fba125cf","3c75b7c0e6a0429c8213d091b8e592a9","a9ee7b294a014b2992af06bb3d7f72ec","bdbfb5bf6dd04ebd8772fd0cfec233e3","55a579e25f4e455bb3b192e1b995663b","55a806ff337b4cc6a28ac440a9d0d99e","812d01ce56094378b7b3cceabab0628b","b0729cf5f60046028782ef98da45082e","b48bc08f76534476bb95e337c4c53cb3","a02fd824865b4ad28ff1664b6d560282","44d7cf84773b4e379447211a19c9a5f7","d603aab863d545c3998785fcc0607b20","568ce285bf854fc8a25c8ded007e4062","e776a81d48cb422cbaf2391a2d17b0f9","58b29e8acb884a948e9c2b799268fee8","e1fd9eec86e046d2ac037b663cba0020","97fb424638c94dc88c67527f78b61154","b535ca9366f8424aad3930114fba9800","886cd1647bd94b6f9249b3003be64228","06784eb3612544b9bffefce52fb2e733","935976cee55a427a952c6488952570a9","3eb8171bbd3b418e971e66a3685f058e","25a06268937444018386741479f49fa6","420b12a1172b4aa6844008f10c6b409d","d1eda40b2c934f288ae930556b9c8de7","046be9070dd74c3096d940e71f942fc3","b004b255ab954eea83f565f6a42d4cbc","1db0c963cd374ac9bd37818112fa104e","fbb8ba9f0bf34e688311340e18d4ee84","35b32266fa22441596870177f53fa45e","d0a32c4714414aa88cae5cb3d2f49432","ccba3e39564c40d4aeea4420aaf71ced","00ae86b0da12432b94a6301bc33fd4d8","d6bf50a4ddfe4730a2df289af70f8fc9","8639a51780664ca28620f8afe7ea1daa","b30e8370b9514c2db70e0ab4127b1fe6","26bbffe8d6564d20a0c8faf7608d9c77","0691e9b42f2c4ee3afedee874402c92d","28ed1eedf8d440ef9acafeeb1bebc5e0","a5e594be38e64d52b7ffa3e1197b5116","231bab54425e418a85d865fcae089c37","c9646c38fab04cfa8386fdb5deefd31e","80ab1338132845d0adff998055f0f671","c6e70c4149a14ea58a47fe1683595ff9","20eeea2c14b54616924c4d98c7a13042","a4672007ce9641d4868679bda9147214","85c8fee0cd79450a975c08076292adf1","c4620ed06be64f0e8d1e086533e741ee","767b67244bc54c0ea7a4d2c6ffe28460","8457777ac4584ff391aa8c8b3939b75a","7f03b61f321e46e492bca6a98173edc2","4135743d136440a78e8dc84173c28a3c","f988542e2a7843c794efb833c4c4a3b5","80609f84948f43719b926482de40237f","c43e11e041a44c87be5276e44921f69f","1b56b3b1042646289c151c128349ddf1","88e1dd2ac13e4a7d89c53de88b013126","665edd3c61634e03adf4923eba813d5c","13eb545b08a046fbbd633eaaca02f40b","4b0f8eb959614d959a9b6c1757312dd5","b8c5993f4f844acbbca2a60bb1b68d9f","a8a0fe16800d41b591193d68c7e0ee5b","3943a7d8cd344dc0b4ba63bff74607db","5e111d423f1f423992972fae0eb635fb","8f155f7b4a444022b430e1831e4e78cb","7f68e82e48914682a5f4d197ef6f461d","a8773393456b47958ece9ef93dff8903","1ee7cb3d5ba846c3b0602f1e56d84568","77d6068c39ce418aabd7fb96eb356d72","ff8602ddf5844838a29709a599102203","93fda5810db14697a32d500a60616e24","f8551caaf36847ffbd095e4761efcbee","565cfb8124c441eeaec1c0eb9b7e059a","cd273ebc4dde48878ad05e17d3a518bb","20b7abd9a8914800b4dbb1a12d4c70dc","239195f73b1e456585e9c63c6e8a291c","43fd9741a2fa4186845503be9d28d3c5","89e186b2795c41328d0c7cdd575f2636","4a0d985689e74cefbcaa6451d07a2a8e","c14dc79636ef482381c92b630017630e","ae3c02164fc948acbce50bfe9b414565","932c8d19d7214950b1a93c8747c592a9","485ff042e4384049aff0057b3930a609"]},"id":"xWh20Yp3aZWE","executionInfo":{"status":"error","timestamp":1764583170209,"user_tz":-540,"elapsed":5662393,"user":{"displayName":"성지양","userId":"15077248708045203212"}},"outputId":"c0bffe73-14e1-4114-b40a-833c0e5966e5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading meta-llama/Llama-3.2-1B-Instruct on cuda...\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a1fd7052ce4efea740672cac4c2644"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af11dad551c04cbd9910030671f3c247"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"812d01ce56094378b7b3cceabab0628b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/877 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b535ca9366f8424aad3930114fba9800"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb8ba9f0bf34e688311340e18d4ee84"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e594be38e64d52b7ffa3e1197b5116"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Loading FULL Dataset...\n"]},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f03b61f321e46e492bca6a98173edc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["data/train-00000-of-00001.parquet:   0%|          | 0.00/41.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a0fe16800d41b591193d68c7e0ee5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/3000 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"565cfb8124c441eeaec1c0eb9b7e059a"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting valid samples...\n"]},{"output_type":"stream","name":"stderr","text":["Scanning Dataset: 100%|██████████| 3000/3000 [00:00<00:00, 7504.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total Documents to Process: 6000\n","Tokenizing sentences...\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["Transforming 29946 sentences (Reduce -> Inject)...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1226607573.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1226607573.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;31m# 1. Process Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_full_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1226607573.py\u001b[0m in \u001b[0;36mprocess_full_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# 3. Batch Transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Transforming {len(all_sentences)} sentences (Reduce -> Inject)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m     \u001b[0mreduced_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_transformation_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reduce\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m     \u001b[0minjected_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_transformation_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduced_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inject\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-1226607573.py\u001b[0m in \u001b[0;36mrun_transformation_pipeline\u001b[0;34m(texts, mode)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# Run pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     outputs = pipe(\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0mchat_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1446\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m                 )\n\u001b[0;32m-> 1448\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2563\u001b[0m         \u001b[0;31m# 9. Call generation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2564\u001b[0;31m         result = decoding_method(\n\u001b[0m\u001b[1;32m   2565\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2566\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2777\u001b[0m             \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_unfinished_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_peer_finished\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynced_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m             \u001b[0;31m# prepare model inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["Based on the provided skeleton code and .csv file, write a complete, executable Python script to perform Human vs. AI text detection. Use the same model as provided, and leave the API key/login section blank as I will input it later.\n","\n","\n","\n","Follow these strict directives for the implementation:\n","\n","\n","\n","1.  Reduced Essay Generation: Create a \"Reduced Essay\" from the Original Essay. You must maintain the exact same sentence count as the original, but simplify each sentence to its minimal semantic units (e.g., Subject-Verb-Object), removing secondary elements like modifiers.\n","\n","\n","\n","2.  Injected Essay Generation: Create an \"Injected Essay\" based on the Reduced Essay. Again, maintain the exact same sentence count, but enrich the sentences by adding modifiers, adjectives, and details to make the description more vivid.\n","\n","\n","\n","3.  Attention Map Calculation: Generate Attention Maps for two transformations:\n","\n","    * Original Essay → Reduced Essay\n","\n","    * Reduced Essay → Injected Essay\n","\n","    * Constraint: Since word counts within sentences vary, calculate the mean of the attention sums for each sentence. This ensures the attention map dimensions align with the sentence count.\n","\n","\n","\n","4.  Detection & Analysis: Compare and analyze the two Attention Maps generated above to classify whether the essay is written by a Human or AI.\n","\n","\n","\n","5.  Outputs: The code must return/display both Attention Maps (Original→Reduced and Reduced→Injected).\n","\n","\n","\n","6.  Implementation Details: You are responsible for filling in the specific technical details to make this flow functional. Include detailed comments in the code explaining the specific logic, model choices, and methods applied at each step."],"metadata":{"id":"LiRrHIN7WD96"}},{"cell_type":"markdown","source":["위 코드는 모델이 정상 작동하는지 확인하기 위해 사용한 skeleton code야. login은 제가 나중에 직접 키를 입력할 예정이니 비워.\n","\n","\n","\n","나는 지금 제공한 .csv 파일을 이용해 아래 작업들을 수행하고자 하는데, 이를 위해 다음 지침에 따라 실행 가능한 전체 코드를 작성해.\n","\n","\n","\n","0. 이 코드의 목표는 입력된 에세이를 바탕으로 Human vs. AI text detection를 수행해.\n","\n","\n","\n","1. 각 에세이의 문장 개수는 원본과 정확히 동일하게 유지하되, 문장을 최소한의 의미 단위(예: 주어, 동사, 목적어)만 남기고 부수적인 요소(예: 수식어)는 제거하여 간소화해. 이를 Reduced Essay라고 칭할 꺼야.\n","\n","\n","\n","2. 앞서 만든 Reduced Essay를 바탕으로, 역시 문장 개수는 그대로 유지하면서 수식어나 형용사 등을 추가하여 문장의 묘사를 더 풍부하고 상세하게 만들어. 이게 Injected Essay야.\n","\n","\n","\n","3. Attention Map을 이용한 탐지 Original Essay -> Reduced Essay 변환 과정에서 생성된 Attention Map과, Reduced Essay -> Injected Essay 변환 과정에서 생성된 Attention Map을 서로 비교 분석하여 인간이 쓴 글인지 AI가 쓴 글인지를 판별할 꺼야.\n","\n","\n","\n","4. 구현 세부 사항 위 내용은 전체적인 흐름으로, 구체적인 기술적 디테일은 너가 채워서 완성된 코드를 제공해. 각 단계별로 구체적으로 어떤 방식이나 로직을 적용했는지 설명하는 상세한 주석을 코드에 적어.\n","\n","\n","\n","5. attention map을 만들 때 각 문장마다 단어의 개수가 일관되지 않을 수 있기 때문에 각 문장에 있는 attention의 합의 평균으로 할 꺼야. 그러면 문장 개수는 유지되니까 작동 가능하겠지.\n","\n","\n","\n","6. 그러면 original essay -> reduced essay의 attention map과, reduced essay -> injected essay의 attention map 둘 다 제공해.\n","\n","\n","\n","이걸 지시하는 영어 문장으로 바꿔서, 바로 ctrl + c, v 가능하게 번역해서 제공해. 부탁이 아니라 지시야."],"metadata":{"id":"f1ASznwCWwxB"}},{"cell_type":"markdown","source":["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"],"metadata":{"id":"oR7P8RTxaHNG"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","from datasets import load_dataset\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import nltk\n","import logging\n","import sys\n","import os\n","from tqdm import tqdm\n","import random\n","\n","# --- Setup & Configuration ---\n","NUM_ROWS_TO_USE = 100  # Strict limit: Process exactly this many rows (e.g., 100 rows -> 200 samples)\n","\n","# 1. Login\n","hf_token = \"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\"\n","\n","if hf_token:\n","    try:\n","        login(token=hf_token)\n","    except Exception as e:\n","        print(f\"Warning: Login failed. {e}\")\n","else:\n","    print(\"No HF token provided. Running anonymously.\")\n","\n","# 2. Model Configuration\n","model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","if torch.backends.mps.is_available():\n","    device = \"mps\"\n","\n","print(f\"Loading {model_id} on {device}...\")\n","\n","try:\n","    # Load Tokenizer\n","    tokenizer_kwargs = {\"token\": hf_token} if hf_token else {}\n","    tokenizer = AutoTokenizer.from_pretrained(model_id, **tokenizer_kwargs)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"left\"\n","\n","    # Load Model\n","    # Note: Using 'torch_dtype' as it is the correct argument for Hugging Face AutoModel.\n","    model_kwargs = {\"token\": hf_token} if hf_token else {}\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id,\n","        dtype=torch.bfloat16,\n","        device_map=\"auto\",\n","        output_hidden_states=True,\n","        **model_kwargs\n","    )\n","except Exception as e:\n","    print(f\"CRITICAL ERROR loading model: {e}\")\n","    sys.exit(1)\n","\n","# Initialize Generation Pipeline\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    device_map=\"auto\",\n","    pad_token_id=tokenizer.eos_token_id,\n","    batch_size=32\n",")\n","\n","# NLTK Setup\n","try:\n","    nltk.data.find('tokenizers/punkt_tab')\n","except LookupError:\n","    nltk.download('punkt_tab')\n","try:\n","    nltk.data.find('tokenizers/punkt')\n","except LookupError:\n","    nltk.download('punkt')\n","\n","# --- MLP Classifier Definition ---\n","\n","class ArtifactDetectorMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(ArtifactDetectorMLP, self).__init__()\n","        # Input dim will be 2 * Hidden_Size (Original + Injected)\n","        self.layer1 = nn.Linear(input_dim, 512)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","        self.layer2 = nn.Linear(512, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.layer2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# --- Helper Functions ---\n","\n","def get_sentences(text):\n","    \"\"\"Returns ALL sentences from the text (No slicing).\"\"\"\n","    if not text or not isinstance(text, str):\n","        return []\n","    return nltk.tokenize.sent_tokenize(text)\n","\n","def run_transformation_pipeline(texts, mode, description=\"Transforming\"):\n","    \"\"\"\n","    Runs the pipeline using a GENERATOR to maximize GPU throughput.\n","    \"\"\"\n","    tokenizer.padding_side = \"left\"\n","\n","    if mode == \"reduce\":\n","        system_prompt = (\n","            \"Task: Simplify the sentence. Keep the main Subject, Verb, and Object.\\n\"\n","            \"Rule: Do not change the meaning. Do not loop. Remove extra details.\\n\"\n","            \"Input Sentence: \"\n","        )\n","    elif mode == \"inject\":\n","        system_prompt = (\n","            \"Task: Rewrite the sentence to be more descriptive and vivid.\\n\"\n","            \"Rule: Add adjectives and adverbs. Keep the original meaning.\\n\"\n","            \"Input Sentence: \"\n","        )\n","\n","    # 1. Prepare Inputs\n","    chat_inputs = [\n","        [\n","            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n","            {\"role\": \"user\", \"content\": system_prompt + t}\n","        ] for t in texts\n","    ]\n","\n","    # 2. Define Generator\n","    def input_generator():\n","        for item in chat_inputs:\n","            yield item\n","\n","    results = []\n","\n","    # 3. Iterate over the pipeline output\n","    pipeline_iterator = pipe(\n","        input_generator(),\n","        batch_size=32,\n","        max_new_tokens=64,\n","        do_sample=False\n","    )\n","\n","    # Use tqdm on the iterator for visual feedback\n","    for out in tqdm(pipeline_iterator, total=len(texts), desc=description):\n","        try:\n","            generated_conv = out[0]['generated_text']\n","            content = generated_conv[-1]['content']\n","\n","            if \"Input Sentence:\" in content:\n","                content = content.split(\"Input Sentence:\")[-1].strip()\n","            results.append(content.strip())\n","        except Exception:\n","            results.append(\"\")\n","\n","    return results\n","\n","def get_embedding_smart_chunking(text, max_tokens=512):\n","    \"\"\"\n","    Calculates document embedding using Sentence-Aware Smart Chunking.\n","    \"\"\"\n","    if not text:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    sentences = get_sentences(text)\n","    if not sentences:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    # 1. Create Chunks\n","    chunks = []\n","    current_chunk = []\n","    current_length = 0\n","\n","    tokenizer.padding_side = \"right\"\n","\n","    for sent in sentences:\n","        token_len = len(tokenizer.encode(sent, add_special_tokens=False))\n","\n","        if current_length + token_len > max_tokens and current_chunk:\n","            chunks.append(\" \".join(current_chunk))\n","            current_chunk = []\n","            current_length = 0\n","\n","        current_chunk.append(sent)\n","        current_length += token_len\n","\n","    if current_chunk:\n","        chunks.append(\" \".join(current_chunk))\n","\n","    # 2. Embed Chunks\n","    chunk_embeddings = []\n","\n","    batch_size = 4\n","    for i in range(0, len(chunks), batch_size):\n","        batch_texts = chunks[i : i + batch_size]\n","\n","        inputs = tokenizer(\n","            batch_texts,\n","            return_tensors=\"pt\",\n","            padding=True,\n","            truncation=True,\n","            max_length=max_tokens\n","        ).to(device)\n","\n","        with torch.no_grad():\n","            outputs = model(**inputs)\n","\n","        last_hidden = outputs.hidden_states[-1]\n","        mask = inputs['attention_mask'].unsqueeze(-1)\n","\n","        sum_emb = torch.sum(last_hidden * mask, dim=1)\n","        counts = torch.clamp(mask.sum(dim=1), min=1e-9)\n","        mean_emb = sum_emb / counts\n","\n","        chunk_embeddings.append(mean_emb)\n","\n","    # 3. Average across chunks\n","    if not chunk_embeddings:\n","        return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    all_chunks_tensor = torch.cat(chunk_embeddings, dim=0)\n","    doc_embedding = torch.mean(all_chunks_tensor, dim=0, keepdim=True)\n","\n","    return doc_embedding\n","\n","# --- Dataset Processing (Strict & Balanced) ---\n","\n","def process_full_dataset():\n","    print(f\"\\n Loading FULL Dataset...\")\n","    try:\n","        ds = load_dataset(\"artnitolog/llm-generated-texts\", split=\"train\")\n","    except Exception as e:\n","        print(f\"Error loading dataset: {e}\")\n","        return None, None\n","\n","    # 1. Collect Raw Data (Strict 1:1 Logic)\n","    raw_samples = []\n","\n","    # Exclude metadata columns\n","    excluded_cols = {'id', 'prompt', 'dataset_name', 'classes'}\n","\n","    # Counter for Processed Source Rows\n","    rows_processed_count = 0\n","\n","    print(f\"Collecting valid samples (Target: {NUM_ROWS_TO_USE} Rows)...\")\n","\n","    for row in tqdm(ds, desc=\"Scanning Dataset\"):\n","        # Check Stop Condition\n","        if NUM_ROWS_TO_USE is not None and rows_processed_count >= NUM_ROWS_TO_USE:\n","            break\n","\n","        all_cols = row.keys()\n","\n","        # Identify Human Column\n","        human_col = next((c for c in all_cols if 'human' in c.lower()), None)\n","        if not human_col: continue # Skip if no human text found\n","\n","        # Identify AI Candidate Columns\n","        ai_candidates = []\n","        for col in all_cols:\n","            if col in excluded_cols: continue\n","            if col == human_col: continue\n","\n","            # Ensure the column has content\n","            if row[col] and isinstance(row[col], str) and len(row[col]) > 10:\n","                ai_candidates.append(col)\n","\n","        if not ai_candidates: continue # Skip if no AI text found\n","        if not row[human_col]: continue\n","\n","        # --- Balanced Sampling Logic ---\n","\n","        # 1. Add Human Sample (Label 0)\n","        human_text = row[human_col]\n","        raw_samples.append((human_text, 0))\n","\n","        # 2. Add Random AI Sample (Label 1)\n","        selected_ai_col = random.choice(ai_candidates)\n","        ai_text = row[selected_ai_col]\n","        raw_samples.append((ai_text, 1))\n","\n","        # Increment Row Counter (1 Row = 1 Human + 1 AI)\n","        rows_processed_count += 1\n","\n","    print(f\"\\nValidation:\")\n","    print(f\"Target Rows: {NUM_ROWS_TO_USE}\")\n","    print(f\"Processed Rows: {rows_processed_count}\")\n","    print(f\"Total Samples Collected: {len(raw_samples)} (Should be {2 * rows_processed_count})\")\n","    print(f\"Human Samples: {sum(1 for _, l in raw_samples if l == 0)}\")\n","    print(f\"AI Samples: {sum(1 for _, l in raw_samples if l == 1)}\")\n","\n","    # 2. Flatten into Sentences\n","    all_sentences = []\n","    doc_boundaries = []\n","    current_idx = 0\n","    valid_samples = []\n","\n","    print(\"Tokenizing sentences (FULL TEXT)...\")\n","    for text, label in raw_samples:\n","        sents = get_sentences(text) # ALL sentences\n","        if not sents: continue\n","\n","        all_sentences.extend(sents)\n","        doc_boundaries.append((current_idx, current_idx + len(sents)))\n","        valid_samples.append((text, label))\n","        current_idx += len(sents)\n","\n","    # 3. Batch Transformation\n","    reduced_sentences = run_transformation_pipeline(\n","        all_sentences,\n","        \"reduce\",\n","        description=\"Step 1/2: Reducing\"\n","    )\n","\n","    injected_sentences = run_transformation_pipeline(\n","        reduced_sentences,\n","        \"inject\",\n","        description=\"Step 2/2: Injecting\"\n","    )\n","\n","    # 4. Feature Extraction\n","    print(\"\\nCalculating Smart Chunked Embeddings...\")\n","    features_list = []\n","    labels_list = []\n","\n","    for i, (start, end) in enumerate(tqdm(doc_boundaries, desc=\"Embedding Docs\")):\n","        inj_segment = \" \".join(injected_sentences[start:end])\n","        orig_text = valid_samples[i][0]\n","\n","        emb_orig = get_embedding_smart_chunking(orig_text)\n","        emb_inj = get_embedding_smart_chunking(inj_segment)\n","\n","        feat = torch.cat((emb_orig, emb_inj), dim=1)\n","\n","        features_list.append(feat.cpu())\n","        labels_list.append(valid_samples[i][1])\n","\n","    if not features_list:\n","        return None, None\n","\n","    X = torch.cat(features_list, dim=0)\n","    y = torch.tensor(labels_list, dtype=torch.float32).unsqueeze(1)\n","\n","    return X, y\n","\n","# --- Training & Evaluation ---\n","\n","def train_and_evaluate(X, y):\n","    print(\"\\n Splitting Data (60/20/20)...\")\n","    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n","    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","    print(f\"Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}\")\n","\n","    X_train, y_train = X_train.to(device), y_train.to(device)\n","    X_val, y_val = X_val.to(device), y_val.to(device)\n","    X_test, y_test = X_test.to(device), y_test.to(device)\n","\n","    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)\n","\n","    input_dim = X.shape[1]\n","    mlp = ArtifactDetectorMLP(input_dim).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = optim.Adam(mlp.parameters(), lr=0.0005)\n","\n","    epochs = 20\n","    best_val_acc = 0.0\n","\n","    print(\"\\n Starting Training...\")\n","\n","    for epoch in range(epochs):\n","        mlp.train()\n","        train_loss = 0\n","\n","        for batch_X, batch_y in train_loader:\n","            optimizer.zero_grad()\n","            outputs = mlp(batch_X)\n","            loss = criterion(outputs, batch_y)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","\n","        mlp.eval()\n","        with torch.no_grad():\n","            val_out = mlp(X_val)\n","            val_loss = criterion(val_out, y_val).item()\n","            val_pred = (val_out > 0.5).float()\n","            val_acc = (val_pred == y_val).sum().item() / len(y_val) * 100\n","\n","        print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n","\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            torch.save(mlp.state_dict(), \"best_model.pth\")\n","\n","    print(f\"\\n Training Complete. Best Val Acc: {best_val_acc:.2f}%\")\n","\n","    print(\"\\n Evaluating on Test Set...\")\n","    mlp.load_state_dict(torch.load(\"best_model.pth\", weights_only=True))\n","    mlp.eval()\n","\n","    with torch.no_grad():\n","        test_out = mlp(X_test)\n","        test_pred = (test_out > 0.5).float()\n","        test_acc = (test_pred == y_test).sum().item() / len(y_test) * 100\n","\n","    print(f\"{'='*30}\")\n","    print(f\"FINAL TEST ACCURACY: {test_acc:.2f}%\")\n","    print(f\"{'='*30}\")\n","\n","    return mlp\n","\n","def main():\n","    # 1. Process Data\n","    X, y = process_full_dataset()\n","\n","    if X is None:\n","        print(\"Data preparation failed.\")\n","        return\n","\n","    # 2. Train & Eval\n","    train_and_evaluate(X, y)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMBFLdFtXbYw","executionInfo":{"status":"ok","timestamp":1764588806521,"user_tz":-540,"elapsed":4068158,"user":{"displayName":"성지양","userId":"15077248708045203212"}},"outputId":"8ca80d27-4b77-4510-83dd-507b15275746"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading meta-llama/Llama-3.2-1B-Instruct on cuda...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Loading FULL Dataset...\n","Collecting valid samples (Target: 100 Rows)...\n"]},{"output_type":"stream","name":"stderr","text":["Scanning Dataset:   3%|▎         | 100/3000 [00:00<00:01, 1811.59it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Validation:\n","Target Rows: 100\n","Processed Rows: 100\n","Total Samples Collected: 200 (Should be 200)\n","Human Samples: 100\n","AI Samples: 100\n","Tokenizing sentences (FULL TEXT)...\n"]},{"output_type":"stream","name":"stderr","text":["\n","Step 1/2: Reducing:   0%|          | 0/5625 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n","Step 1/2: Reducing: 100%|██████████| 5625/5625 [25:58<00:00,  3.61it/s]\n","Step 2/2: Injecting: 100%|██████████| 5625/5625 [30:11<00:00,  3.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Calculating Smart Chunked Embeddings...\n"]},{"output_type":"stream","name":"stderr","text":["Embedding Docs: 100%|██████████| 200/200 [10:56<00:00,  3.28s/it]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Splitting Data (60/20/20)...\n","Train: 120 | Val: 40 | Test: 40\n","\n"," Starting Training...\n","Epoch [1/20] | Train Loss: 0.6718 | Val Loss: 0.5559 | Val Acc: 67.50%\n","Epoch [2/20] | Train Loss: 0.3261 | Val Loss: 0.2011 | Val Acc: 95.00%\n","Epoch [3/20] | Train Loss: 0.1249 | Val Loss: 0.0840 | Val Acc: 100.00%\n","Epoch [4/20] | Train Loss: 0.0829 | Val Loss: 0.0763 | Val Acc: 95.00%\n","Epoch [5/20] | Train Loss: 0.0546 | Val Loss: 0.0873 | Val Acc: 95.00%\n","Epoch [6/20] | Train Loss: 0.0263 | Val Loss: 0.0355 | Val Acc: 100.00%\n","Epoch [7/20] | Train Loss: 0.0305 | Val Loss: 0.0271 | Val Acc: 100.00%\n","Epoch [8/20] | Train Loss: 0.0158 | Val Loss: 0.0588 | Val Acc: 95.00%\n","Epoch [9/20] | Train Loss: 0.0220 | Val Loss: 0.0246 | Val Acc: 100.00%\n","Epoch [10/20] | Train Loss: 0.0090 | Val Loss: 0.0219 | Val Acc: 100.00%\n","Epoch [11/20] | Train Loss: 0.0151 | Val Loss: 0.0175 | Val Acc: 100.00%\n","Epoch [12/20] | Train Loss: 0.0061 | Val Loss: 0.0297 | Val Acc: 100.00%\n","Epoch [13/20] | Train Loss: 0.0057 | Val Loss: 0.0387 | Val Acc: 97.50%\n","Epoch [14/20] | Train Loss: 0.0042 | Val Loss: 0.0333 | Val Acc: 100.00%\n","Epoch [15/20] | Train Loss: 0.0046 | Val Loss: 0.0228 | Val Acc: 100.00%\n","Epoch [16/20] | Train Loss: 0.0033 | Val Loss: 0.0204 | Val Acc: 100.00%\n","Epoch [17/20] | Train Loss: 0.0029 | Val Loss: 0.0201 | Val Acc: 100.00%\n","Epoch [18/20] | Train Loss: 0.0033 | Val Loss: 0.0223 | Val Acc: 100.00%\n","Epoch [19/20] | Train Loss: 0.0029 | Val Loss: 0.0247 | Val Acc: 100.00%\n","Epoch [20/20] | Train Loss: 0.0036 | Val Loss: 0.0236 | Val Acc: 100.00%\n","\n"," Training Complete. Best Val Acc: 100.00%\n","\n"," Evaluating on Test Set...\n","==============================\n","FINAL TEST ACCURACY: 100.00%\n","==============================\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n","from huggingface_hub import login\n","from datasets import load_dataset\n","import nltk\n","import sys\n","import os\n","import random\n","import time\n","from IPython.display import clear_output  # 화면 갱신용\n","\n","# --- Configuration ---\n","MODEL_PATH = \"best_model.pth\"\n","hf_token = \"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\"\n","\n","# 1. Login\n","if hf_token:\n","    try:\n","        login(token=hf_token)\n","    except Exception as e:\n","        print(f\"Warning: Login failed. {e}\")\n","\n","# 2. Model Setup (GLOBAL SCOPE - 무조건 실행됨)\n","model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Loading {model_id} on {device}...\")\n","try:\n","    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    tokenizer.padding_side = \"left\"\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_id, dtype=torch.bfloat16, device_map=\"auto\", output_hidden_states=True, token=hf_token\n","    )\n","except Exception as e:\n","    print(f\"Error: {e}\")\n","    sys.exit(1)\n","\n","# 파이프라인도 전역으로 설정\n","pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\", pad_token_id=tokenizer.eos_token_id, batch_size=32)\n","\n","# --- MLP Classifier Structure ---\n","class ArtifactDetectorMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super(ArtifactDetectorMLP, self).__init__()\n","        self.layer1 = nn.Linear(input_dim, 512)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.3)\n","        self.layer2 = nn.Linear(512, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        x = self.layer2(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","# --- Helper Functions ---\n","def get_sentences(text):\n","    if not text or not isinstance(text, str): return []\n","    return nltk.tokenize.sent_tokenize(text)\n","\n","def run_transformation_pipeline(texts, mode):\n","    tokenizer.padding_side = \"left\"\n","    if mode == \"reduce\":\n","        sys_prompt = \"Task: Simplify. Keep Subject, Verb, Object. Rule: No meaning change. Input: \"\n","    else:\n","        sys_prompt = \"Task: Rewrite to be descriptive. Rule: Add adjectives. Input: \"\n","\n","    chat_inputs = [[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": sys_prompt + t}] for t in texts]\n","    results = []\n","\n","    batch_size = 32\n","    for i in range(0, len(chat_inputs), batch_size):\n","        batch = chat_inputs[i : i + batch_size]\n","        outputs = pipe(batch, max_new_tokens=64, do_sample=False)\n","        for out in outputs:\n","            try:\n","                c = out[0]['generated_text'][-1]['content']\n","                if \"Input:\" in c: c = c.split(\"Input:\")[-1]\n","                elif \"Input Sentence:\" in c: c = c.split(\"Input Sentence:\")[-1]\n","                results.append(c.strip())\n","            except: results.append(\"\")\n","    return results\n","\n","def get_embedding_smart_chunking(text, max_tokens=512):\n","    if not text: return torch.zeros((1, model.config.hidden_size), device=device)\n","    sentences = get_sentences(text)\n","    if not sentences: return torch.zeros((1, model.config.hidden_size), device=device)\n","\n","    chunks, cur_chunk, cur_len = [], [], 0\n","    tokenizer.padding_side = \"right\"\n","    for s in sentences:\n","        l = len(tokenizer.encode(s, add_special_tokens=False))\n","        if cur_len + l > max_tokens and cur_chunk:\n","            chunks.append(\" \".join(cur_chunk)); cur_chunk = []; cur_len = 0\n","        cur_chunk.append(s); cur_len += l\n","    if cur_chunk: chunks.append(\" \".join(cur_chunk))\n","\n","    embs = []\n","    for i in range(0, len(chunks), 4):\n","        inps = tokenizer(chunks[i:i+4], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_tokens).to(device)\n","        with torch.no_grad(): out = model(**inps)\n","        mask = inps['attention_mask'].unsqueeze(-1)\n","        embs.append((out.hidden_states[-1] * mask).sum(1) / torch.clamp(mask.sum(1), min=1e-9))\n","\n","    return torch.mean(torch.cat(embs, 0), dim=0, keepdim=True)\n","\n","# --- Statistics & Dashboard Class ---\n","class Stats:\n","    def __init__(self):\n","        self.h_total = 0\n","        self.h_hit = 0\n","        self.a_total = 0\n","        self.a_hit = 0\n","\n","        # 기록: Human인데 AI 점수가 가장 높게 나온 경우 (Worst Human)\n","        self.max_human_prob = 0.0\n","        # 기록: AI인데 AI 점수가 가장 낮게 나온 경우 (Worst AI)\n","        self.min_ai_prob = 1.0\n","\n","    def update(self, is_human, is_correct, prob):\n","        if is_human:\n","            self.h_total += 1\n","            if is_correct: self.h_hit += 1\n","            if prob > self.max_human_prob: self.max_human_prob = prob\n","        else:\n","            self.a_total += 1\n","            if is_correct: self.a_hit += 1\n","            if prob < self.min_ai_prob: self.min_ai_prob = prob\n","\n","    def print_dashboard(self, current_row, current_type, current_prob, sent_len):\n","        clear_output(wait=True) # 화면 갱신\n","\n","        h_miss = self.h_total - self.h_hit\n","        a_miss = self.a_total - self.a_hit\n","\n","        # 0 나누기 방지\n","        h_acc = (self.h_hit / self.h_total * 100) if self.h_total > 0 else 0\n","        a_acc = (self.a_hit / self.a_total * 100) if self.a_total > 0 else 0\n","\n","        print(f\"Processing Row: {current_row} | Type: [{current_type}] ({sent_len} sents) | Prob: {current_prob:.4f}\")\n","        print(\"=\"*60)\n","        print(f\" HUMAN HIT  : {self.h_hit} / {self.h_total} ({h_acc:.1f}%)\")\n","        print(f\" HUMAN MISS : {h_miss} / {self.h_total}\")\n","        print(f\" >> Highest Prob (Worst Human): {self.max_human_prob:.4f}\")\n","        print(\"-\" * 60)\n","        print(f\" AI HIT     : {self.a_hit} / {self.a_total} ({a_acc:.1f}%)\")\n","        print(f\" AI MISS    : {a_miss} / {self.a_total}\")\n","        print(f\" >> Lowest Prob (Worst AI)    : {self.min_ai_prob:.4f}\")\n","        print(\"=\"*60)\n","        print(\"Press Stop Button (or Ctrl+C) to finish.\")\n","\n","# --- Main Logic ---\n","def main():\n","    # 1. Load MLP\n","    print(f\"\\nLoading trained model from {MODEL_PATH}...\")\n","    try:\n","        input_dim = model.config.hidden_size * 2\n","        mlp = ArtifactDetectorMLP(input_dim).to(device)\n","        mlp.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n","        mlp.eval()\n","        print(\"Model loaded successfully.\")\n","    except FileNotFoundError:\n","        print(\"Error: best_model.pth not found. Please train first.\")\n","        return\n","\n","    # 2. Load Dataset\n","    print(\"Loading Dataset...\")\n","    ds = load_dataset(\"artnitolog/llm-generated-texts\", split=\"train\")\n","\n","    stats = Stats()\n","\n","    print(\"\\nStarting Real-time Inference...\")\n","\n","    try:\n","        for i, row in enumerate(ds):\n","\n","            # 1. Identify Human Column\n","            col_human = next((c for c in row.keys() if 'human' in c.lower()), None)\n","            if not col_human or not row[col_human]: continue\n","\n","            # 2. Identify ALL AI Columns (Exclude metadata)\n","            exclude_cols = ['id', 'prompt', 'dataset_name', 'classes', col_human]\n","            ai_cols = [c for c in row.keys() if c not in exclude_cols and row[c]]\n","\n","            # 3. Targets: (Text, Label, TypeStr)\n","            # Human 1개\n","            targets = [(row[col_human], 0, \"Human\")]\n","            # AI 모두 (Random Choice 아님 -> All)\n","            for ac in ai_cols:\n","                targets.append((row[ac], 1, f\"AI-{ac}\"))\n","\n","            # 4. Process\n","            for text, true_label, type_str in targets:\n","                sents = get_sentences(text)\n","                if not sents: continue\n","\n","                # --- Pipeline Execution ---\n","                red = run_transformation_pipeline(sents, \"reduce\")\n","                inj = run_transformation_pipeline(red, \"inject\")\n","                inj_text = \" \".join(inj)\n","\n","                # --- Prediction ---\n","                with torch.no_grad():\n","                    emb_orig = get_embedding_smart_chunking(text)\n","                    emb_inj = get_embedding_smart_chunking(inj_text)\n","                    feat = torch.cat((emb_orig, emb_inj), dim=1)\n","                    prob = mlp(feat).item()\n","                    pred_label = 1 if prob > 0.5 else 0\n","\n","                # --- Update Dashboard ---\n","                is_correct = (pred_label == true_label)\n","                stats.update(is_human=(true_label==0), is_correct=is_correct, prob=prob)\n","\n","                stats.print_dashboard(\n","                    current_row=i,\n","                    current_type=type_str,\n","                    current_prob=prob,\n","                    sent_len=len(sents)\n","                )\n","\n","    except KeyboardInterrupt:\n","        print(\"\\n\\nStopped by User.\")\n","        stats.print_dashboard(\n","            current_row=\"STOPPED\", current_type=\"N/A\", current_prob=0.0, sent_len=0\n","        )\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWmHHljSuKgX","executionInfo":{"status":"ok","timestamp":1764591990349,"user_tz":-540,"elapsed":2135077,"user":{"displayName":"성지양","userId":"15077248708045203212"}},"outputId":"5144b653-e4f9-4c3e-e2e2-931e5312f74c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing Row: STOPPED | Type: [N/A] (0 sents) | Prob: 0.0000\n","============================================================\n"," HUMAN HIT  : 14 / 14 (100.0%)\n"," HUMAN MISS : 0 / 14\n"," >> Highest Prob (Worst Human): 0.0790\n","------------------------------------------------------------\n"," AI HIT     : 96 / 96 (100.0%)\n"," AI MISS    : 0 / 96\n"," >> Lowest Prob (Worst AI)    : 0.6888\n","============================================================\n","Press Stop Button (or Ctrl+C) to finish.\n"]}]}]}